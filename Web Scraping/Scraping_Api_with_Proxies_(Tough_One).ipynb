{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoWmeuMkbELioHHrRdEE2/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faisal-fida/100-Python-Projects-in-Google-Colab/blob/main/Scraping_Api_with_Proxies_(Tough_One).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1VcfwiYVtm0"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import string\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "proxy = {'http': 'http://2.56.19.91:1074'}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def requestsMe(proxyLabel: bool, URL: str):\n",
        "    with requests.Session() as session:\n",
        "      if proxyLabel == True:\n",
        "        session.proxies = proxy\n",
        "      resp = session.get(URL, proxies=proxy)\n",
        "    if resp.status_code == 200:\n",
        "      soup = BeautifulSoup(resp.content, 'html.parser')\n",
        "      return soup\n",
        "    else:\n",
        "      print(f\"Error at {URL}\")\n",
        "      return None"
      ],
      "metadata": {
        "id": "CIk4qU7kkgG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script 1:"
      ],
      "metadata": {
        "id": "E7T02TVtrWDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soups = []\n",
        "\n",
        "alphabets = list(string.ascii_uppercase)\n",
        "\n",
        "for i in (alphabets):\n",
        "  url = URL.format(i)\n",
        "\n",
        "\n",
        "  soup = requestsMe(proxyLabel=False, URL = URL)\n",
        "  soups.append(soup)"
      ],
      "metadata": {
        "id": "89FlM0pwaKXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "URL = 'https://www.example.com/alphabet={}'\n",
        "alphabets = list(string.ascii_uppercase)\n",
        "\n",
        "URLS = [URL.format(i) for i in (alphabets)]"
      ],
      "metadata": {
        "id": "_f8GHQgxmKXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in URLS:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "3kWC_sBomtl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script 2:"
      ],
      "metadata": {
        "id": "iKiRoAaorYjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "def get_data(soup):\n",
        "  text = soup.select('script')[-1].get_text()\n",
        "  Reresult = re.search(r'\"Data\":(\\[.*?\\]),', text)\n",
        "  result = json.loads(Reresult.group(1))\n",
        "  results.append(result)"
      ],
      "metadata": {
        "id": "OKoyGK3KpOA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, soup in enumerate(soups):\n",
        "  get_data(soup)\n",
        "  print(idx)"
      ],
      "metadata": {
        "id": "AlvqajTcpHEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soups[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEI0BLaiptIs",
        "outputId": "d998b632-a988-47ab-ac17-f341aaefb874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<div style=\"text-align:center; margin: 20px;\">No Regulatory Actions Issued Since January 1, 2016.</div>"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = soups[0].select('script')[-1].get_text()\n",
        "result_Re = re.search(r'\"Data\":(\\[.*?\\]),', text)\n",
        "resultJson = json.loads(result_Re.group(1))"
      ],
      "metadata": {
        "id": "Q4luNi29ad_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agency_items = soup.select('div.agency_items')\n",
        "datas = []\n",
        "\n",
        "for agency_item in agency_items:\n",
        "  data = {}\n",
        "  div_row = agency_item.find('div', class_='row')\n",
        "  div_col_md_12 = div_row.find('div', class_='col-md-12')\n",
        "  div_inner_row = div_col_md_12.find('div', class_='row')\n",
        "\n",
        "  data['link_text'] = div_inner_row.find('div', class_='col-md-7').text\n",
        "  data['action_type'] = div_inner_row.find('div', class_='col-md-3').text\n",
        "  data['date'] = div_inner_row.find('div', class_='col-md-2').text\n",
        "  datas.append(data)"
      ],
      "metadata": {
        "id": "3UAHrIJ4gWjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame().from_dict(datas)\n",
        "df.to_csv('out.csv')"
      ],
      "metadata": {
        "id": "eBOMKaciVvUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_html(URL)\n",
        "df[0].to_csv('out.csv')"
      ],
      "metadata": {
        "id": "1Y-H5PbkX6uw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}