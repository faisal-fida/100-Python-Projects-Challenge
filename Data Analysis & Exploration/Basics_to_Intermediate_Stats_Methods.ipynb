{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/ZeIilTr/Ti6z1xeQVRFv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faisal-fida/100-Python-Projects-in-Google-Colab/blob/main/Basics_to_Intermediate_Stats_Methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Principal Component Analysis (PCA)\n",
        "PCA is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while preserving the most important information.\n",
        "\n",
        "## Support Vector Machines (SVM)\n",
        "SVM is a supervised learning algorithm used for classification and regression tasks. It finds a hyperplane that best separates different classes in the data.\n",
        "\n",
        "## K-Means Clustering\n",
        "K-Means is an unsupervised clustering algorithm that partitions data into 'k' clusters based on similarity, with each cluster represented by its mean.\n",
        "\n",
        "## Hierarchical Clustering\n",
        "Hierarchical Clustering is another unsupervised clustering technique that creates a tree-like structure of data points, allowing for different levels of granularity in clustering.\n",
        "\n",
        "## Naive Bayes\n",
        "Naive Bayes is a probabilistic classification algorithm based on Bayes' theorem. It assumes independence between features, hence the \"naive\" label.\n",
        "\n",
        "## Decision Trees\n",
        "Decision Trees are tree-like structures used for both classification and regression tasks. They split data based on features to make predictions.\n",
        "\n",
        "## Random Forest\n",
        "Random Forest is an ensemble learning method that creates multiple decision trees and combines their outputs for more accurate predictions.\n",
        "\n",
        "## Gradient Boosting\n",
        "Gradient Boosting is another ensemble technique that combines weak learners (usually decision trees) to build a strong predictive model.\n",
        "\n",
        "## Markov Models\n",
        "Markov Models are used to represent sequential data where the future state depends only on the current state, making them useful for time-series analysis.\n",
        "\n",
        "## Hidden Markov Models (HMM)\n",
        "HMM is an extension of Markov Models where the underlying system is assumed to be unobservable. They are widely used in speech recognition and natural language processing.\n",
        "\n",
        "## Gibbs Sampling\n",
        "Gibbs Sampling is a Markov Chain Monte Carlo (MCMC) technique used for sampling from complex probability distributions, often used in Bayesian statistics.\n",
        "\n",
        "## Long Short-Term Memory (LSTM)\n",
        "LSTM is a type of recurrent neural network (RNN) designed to capture long-term dependencies in sequential data, commonly used in natural language processing and time-series analysis.\n",
        "\n",
        "## Gated Recurrent Units (GRU)\n",
        "GRU is another type of RNN that simplifies the LSTM architecture while still capturing temporal dependencies in sequential data.\n",
        "\n",
        "## Autoencoders\n",
        "Autoencoders are neural network architectures used for unsupervised learning and dimensionality reduction by reconstructing input data from a compressed representation.\n",
        "\n",
        "## Variational Autoencoders (VAE)\n",
        "VAE is an extension of autoencoders that incorporates probabilistic modeling, allowing for generative capabilities and learning underlying data distributions.\n",
        "\n",
        "## t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
        "t-SNE is a dimensionality reduction technique used for visualizing high-dimensional data in a lower-dimensional space while preserving pairwise similarities.\n",
        "\n",
        "## UMAP (Uniform Manifold Approximation and Projection)\n",
        "UMAP is another dimensionality reduction technique known for its ability to preserve both local and global structures of the data.\n",
        "\n",
        "## Survival Analysis\n",
        "Survival Analysis is used to analyze time-to-event data, often employed in medical research to model patient survival rates.\n",
        "\n",
        "## Anomaly Detection\n",
        "Anomaly Detection involves identifying rare events or outliers in data, often using statistical methods like clustering, density estimation, or machine learning models.\n",
        "\n",
        "## Bootstrapping\n",
        "Bootstrapping is a resampling technique used to estimate the sampling distribution of a statistic by repeatedly resampling from the observed data.\n",
        "\n",
        "## Cross-Validation\n",
        "Cross-Validation is a technique used to assess the performance of a model by dividing the data into subsets for training and testing, helping to mitigate overfitting.\n",
        "\n",
        "## Bayesian Inference\n",
        "Bayesian Inference is a probabilistic approach to making statistical inferences by updating beliefs as new data becomes available.\n",
        "\n",
        "## Hypothesis Testing\n",
        "Hypothesis Testing is a method for making inferences about population parameters based on sample data, helping to decide if an observed effect is statistically significant.\n",
        "\n",
        "## Regularization\n",
        "Regularization techniques like L1 and L2 regularization help prevent overfitting in machine learning models by adding penalties to the model's parameters.\n"
      ],
      "metadata": {
        "id": "cpYVhOV9madC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from hmmlearn import hmm"
      ],
      "metadata": {
        "id": "ehQEAwjknTxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from tensorflow.keras.layers import LSTM, GRU, Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn import datasets"
      ],
      "metadata": {
        "id": "qYRYNMYdnTzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from scipy import stats"
      ],
      "metadata": {
        "id": "-cWHWHoJnYCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Principal Component Analysis (PCA)\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Support Vector Machines (SVM)\n",
        "svm = SVC()\n",
        "svm.fit(X, y)\n",
        "\n",
        "# K-Means Clustering\n",
        "kmeans = KMeans(n_clusters=3)\n",
        "kmeans_labels = kmeans.fit_predict(X)\n",
        "\n",
        "# Naive Bayes\n",
        "nb = GaussianNB()\n",
        "nb.fit(X, y)\n",
        "\n",
        "# Decision Trees\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X, y)\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Gradient Boosting\n",
        "gb = GradientBoostingClassifier()\n",
        "gb.fit(X, y)\n",
        "\n",
        "# t-SNE\n",
        "tsne = TSNE(n_components=2)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "# Long Short-Term Memory (LSTM)\n",
        "input_layer = Input(shape=(sequence_length, num_features))\n",
        "lstm_layer = LSTM(64)(input_layer)\n",
        "output_layer = Dense(num_classes, activation='softmax')(lstm_layer)\n",
        "lstm_model = Model(inputs=input_layer, outputs=output_layer)\n",
        "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Anomaly Detection using Isolation Forest\n",
        "iso_forest = IsolationForest(contamination=0.1)\n",
        "iso_forest.fit(X)\n",
        "anomaly_scores = iso_forest.decision_function(X)\n",
        "\n",
        "# Cross-Validation\n",
        "svm = SVC()\n",
        "scores = cross_val_score(svm, X, y, cv=5)  # 5-fold cross-validation\n",
        "\n",
        "# Hypothesis Testing (t-test)\n",
        "t_statistic, p_value = stats.ttest_ind(sample_A, sample_B)\n",
        "\n",
        "# Regularization (L2 Regularization with Logistic Regression)\n",
        "X, y = datasets.load_breast_cancer(return_X_y=True)\n",
        "X = StandardScaler().fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "logreg = LogisticRegression(penalty='l2')\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred = logreg.predict(X_test)"
      ],
      "metadata": {
        "id": "9RYMKYFfnRyQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}